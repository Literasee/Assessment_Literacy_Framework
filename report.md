<center> <h1>A Framework for Improving Assessment Literacy</h1> </center>

According to National Council on Teacher Quality (2015), teacher education “preparation programs are not delivering new teachers with needed skills, forcing districts to dedicate professional development dollars to accomplish what they believe higher education should have done in the first place.”  In the Council’s review of 690 teacher education programs, only 24% adequately train teachers how to assess learning and use student performance data to inform instruction and these concepts are embedded in methods classes.   However, none of the programs reviewed offered a data literacy course.  What does this mean?  Pre-service teachers do not have opportunities to wrestle with understanding the data derived from assessments nor to thoroughly understand how to use the data to plan instruction.  These results are after the 2010, NCATE, the largest accreditor of teacher preparation programs, report recommended that “candidates be presented with multiple and rich course material in their preparation that will enable them to become assessment-literate and data-wise.” It did not however, provide specific suggestions as to how this new information could be integrated into existing coursework or provided through new coursework. Despite their current lack of training, classroom teachers report spending as much as one-third to one-half of their time involved in assessment-related activities, whether preparing, administering, or reviewing assessments in order to determine how to make instructional changes (Stiggins, 1992).

In order to compensate for the lack of assessment literacy training in teacher preparation programs, districts attempt to fill this gap through various structures and opportunities such as required professional development, common planning time, or through professional learning communities in which educators meet periodically over an extended period of time to focus on specific topics.  Although these structures and opportunities could assist educators in collaborating when looking at assessment data or analyzing student work, even groups of teachers are at a disadvantage as there is a lack of clarity in what they are looking for and knowing what to do with the information they find. In many cases professional development is simply a statutory requirement and does not necessarily sustain a focus on assessments and data use (William, p. 28).

Coupled with the insufficient understanding of assessment literacy is the controversial belief that schools are over testing students.  Consequently educators are left with the belief that 1) all assessments are tests, 2) an uncertainty as to which assessments should be administered, and 3) the uncertainty about the meaning of the results of the assessments they do administer.  A clear understanding of the critical role that assessments play in the classroom, as well as understanding how assessment data can be used by a variety of different audiences, can assist educators in making decisions about which assessments to select to best serve their instructional needs.

In this paper, we attempt to lay out a framework that will help to develop that clear understanding among teachers, administrators, policymakers and those who are responsible for a) developing instructional materials to improve assessment literacy, b) providing assessment resources to educators, and c) providing pre-service training and in-service professional development related to assessment literacy.  The paper is divided into three main sections. In the first section, we develop a definition of assessment literacy.  In the second section, we provide examples of resources and approaches designed to improve assessment literacy.  In the final section of the paper, we discuss what is needed to build and sustain assessment literacy over the long term.


## Defining Assessment Literacy

A prerequisite to improving assessment literacy is a shared understanding of the meaning of assessment literacy; not only an understating of the content which assessment literacy comprises, but an understanding also of the larger knowledge base which the term assessment literate entails.  When asked to define assessment literacy, there is a natural tendency to begin to list specific assessment-related knowledge, understanding, and skills that an assessment literate educator must possess.  This could be a straightforward list of topics and concepts such as the following selected topics from Michigan Assessment Consortium (MAC) Assessment Literacy Standards for Teachers:

- There are different purposes for student assessment
- A non-technical understanding statistical concepts associated with assessment:
  *	Reliability
  *	Validity, a characteristic of the use of the assessment, not the assessment itself
- Use a variety of protocols for looking at and scoring student work
- Use assessment results to make appropriate instructional decisions for individual students and groups of students.
- Use assessment results appropriately to modify instruction to improve student achievement.  

Alternatively, the list could be expanded to include more detail as was done by Popham (2012) with corresponding items selected from a list of potential assessment literacy topics for educators:

- The fundamental function of educational assessment – namely the collection of evidence from which inferences can be made about students’ covert skills, knowledge, and affect.
- Reliability of educational assessments, especially the three ways in which consistency evidence is reported for groups of test-takers (stability, alternate-form reliability, and internal consistency) and how to gauge the consistency of assessment for individual test takers.
- The prominent role three types of validity evidence (content-related, criterion-related, and construct-related evidence) should play in building arguments to support the accuracy of test-based interpretations.
- Scoring of students’ responses to constructed-response test items, especially the distinctive contribution made by well-formed rubrics.
- Designing and implementing formative assessment procedures consonant with both research evidence and experience-based insights about the likely success of such procedures.
- How to determine the appropriateness of an accountability test for use in evaluating the quality of instruction.

Regardless of the level of detail included, lists of topics are only the starting point in defining assessment literacy.


Popham (2012) offers an elegant definition of assessment literacy:  

> Assessment literacy consists of an individual’s understandings of the fundamental assessment concepts and procedures deemed
> likely to influence educational decisions.

What makes Popham’s definition elegant is that it does not divorce understanding of assessment concepts and procedures from the context in which they will be used; that is, to influence educational decisions.  Many people reading that definition will focus on the words “understandings of the fundamental assessment concepts and procedures” as the critical aspects to identify an assessment literate person.  However, the second half of the definition, “deemed likely to influence educational decisions,” is equally important.  Assessment literacy includes not only an understanding of assessment concepts and procedures, but also the proficiency to apply those concepts and procedures to influence educational decisions within a particular context as a teacher, administrator, or policymaker (or student, parent, taxpayer, …).  

Compare Popham’s definition to the definition of assessment literacy contained within the Assessment Literacy Standards developed by the Michigan Assessment Consortium (MAC):

> Assessment literacy is the set of beliefs and knowledge and practices about assessment that lead a teacher to use
> assessment to improve student learning and achievement.

The MAC definition brings another dimension to the concept of assessment literacy: disposition.  Central to the MAC concept of assessment literacy is the idea that not only does the teacher possess the requisite knowledge and skills to use assessment effectively to improve student learning, but also has the inclination and tendency to do so. On the surface, disposition may not seem central to the definition of literacy or a literate person.  When focused on outcomes (i.e., improved instruction and student learning), however, there can be little argument about the importance of dispositions and the appropriateness of including them in the definition of assessment literacy.  It is not enough to simply understand that not smoking, a proper diet, maintaining a healthy weight, and physical activity are core components of a heart-healthy lifestyle.  It is not enough simply to know that establishing a personal budget, saving and investing wisely, and not accumulating large amounts of credit card debt are core components of financial wellness.  Similarly, understanding assessment concepts and procedures and their role in instruction will not lead to improved instruction and student learning without the disposition to apply that knowledge and skills.

As a starting point to defining assessment literacy, therefore, we began with the fundamental concept of literacy; that is, the possession of basic knowledge and skills (e.g., the ability to read and write).  We then moved from literacy to the notion of a person being literate in a particular area, a concept which includes the knowing of how and when to apply that basic knowledge and skills appropriately within a particular concept.  Finally, we completed the definition with the MAC requirement that an assessment literate educator not only possess an understanding of fundamental assessment concepts and procedures and know how to use them within their specific context, but also have the disposition to use that knowledge and skills to improve instruction and student learning.

Working from that definition, in the following sections, we will discuss context in more detail and suggest an assessment literacy framework to guide the development of assessment literate educators.


## Defining context for teachers, administrators, and policymakers

If assessment literacy is context dependent, then it follows that one must understand the context in which an educator is functioning to begin to understand what is needed to be assessment literate within that particular context.  In this section, we address context for three board categories of educators: teachers, administrators, and policymakers.    Of course, there will be variation in context within those three broad categories.  There are certainly differences among the teaching contexts and assessment literacy needs of an elementary school teacher, a middle school science specialist, and a high school special educator.  There will also variation within the broad class of administrators that might include building level principals, district curriculum directors, and superintendents.  However, there are enough similarities within each of the three categories and enough significant differences across the three categories to illustrate the importance of context in defining and determining how best to improve assessment literacy.  Of course, there are additional stakeholders who could also be considered such as students, parents, and the general public.  Their context for the interpretation and use of assessment is different from that of teachers, administrators, and policymakers.  At this time, however, those groups are beyond the scope of our discussion.

#### Teachers

For teachers, assessment involves gathering and interpreting evidence to inform instructional decisions to improve instruction and learning for individual students.  As suggested in the MAC standards, quality assessments are a critical attribute of effective teaching. It can be argued that at the classroom level assessment and instruction must be tightly interwoven to have the most positive impact on student learning. Therefore, it is logical to identify instruction as the primary context for assessment literacy for teachers.  


As suggested in the figure below, it is also logical to establish a dependent link between assessment literacy and instructional literacy.  That is, the upper bound of teachers’ assessment literacy will always be impacted by their instructional literacy.  If we accept that the definition of an assessment literate teacher includes the capacity to use the information gathered from student assessment, limitations in instructional literacy must be place a cap on the extent to which that teacher can be considered assessment literate .  


![Teacher Context](https://charliedepascale.files.wordpress.com/2016/02/teacher-context.jpg)


Additionally, it should go without saying that teachers’ instructional literacy is similarly bounded by their content literacy.  That is, there are limits to how effective a teacher’s instruction within a particular content area can be without a deep understanding of the content that they are teaching.  Therefore, for teachers, we define assessment literacy within the context of instructional literacy and content literacy.

#### Administrators


In contrast to the instructional context of teachers, the primary context for assessment literacy for administrators is often evaluation – program evaluation and personnel evaluation.  One or more steps removed from instruction, administrators are often using assessment to provide information needed to make judgments regarding the effectiveness of an instructional or curricular program or judgments regarding the relative effectiveness of two or more programs, approaches, or even staff members.
To a certain degree, however, administrators may also need to function within the context of teachers; That may be the case in their role as teaching supervisors and the requirement to observe, evaluate, and provide feedback to improve instruction.  Certain administrators may also be in the position of selecting assessment-related resources and materials for teachers.

![Administrator Context](https://charliedepascale.files.wordpress.com/2016/02/administrator-context.jpg)


#### Policymakers


Educators in our category of policymakers are further removed from the classroom and instruction than administrators.  They may be large-district superintendents, state department of education staff, or United States Department of Education staff.  Policymakers may also include non-educators such as elected officials at the local, state, and federal level.
Distinct from the program and personnel evaluation context of administrators, policymakers are primarily involved assessment for the purpose of data collection for the purpose of program monitoring.  There may be an evaluation aspect to program monitoring, but that evaluation would generally be at a higher-level and more removed from the actual program than evaluation conducted by administrators (e.g., more focused on outcomes and summative judgments).  The four pillars defining the mission of the United States Department of Education provide an example of the context of policymakers:


- Establishing policies on federal financial aid for education, and distributing as well as monitoring those funds.
- Collecting data on America's schools and disseminating research.
- Focusing national attention on key educational issues.
- Prohibiting discrimination and ensuring equal access to education.

![Policy Maker Context](https://charliedepascale.files.wordpress.com/2016/02/policymaker-context.jpg)


#### Context Summary

We have identified three educator-context sets: teacher-instruction, administrator-evaluation, policymaker-data collection.  Those sets are clearly not disjoint.  As suggested throughout, there will be some degree of overlap.  It would be inappropriate, however, to regard the educator categories and contexts as completely crossed.  The assessment literacy skills of an effective teacher will most certainly differ from those of an effective administrator and from those of an effective policymaker.

## Categorizing Assessment Literacy Skills

Reading through any discussion about or list of the assessment concepts, knowledge, and skills that constitute assessment literacy, it is obvious that assessment literacy includes skills from multiple, distinct areas.  For example, within the outline approach of the MAC Assessment Literacy Standards identifies and categorizes those sets of skills to some extent.  Within the overall Knowledge category, there are sections related to knowledge of assessment concepts, knowledge of statistical or measurement concepts, and knowledge of how to effectively interpret, use, and communicate results from assessments.   More often than not, however, knowledge and skills from distinct disciplines are lumped under the general heading of Assessment Literacy.  In this context, we are using the formal definition of the technical term lumped as:

Put in an indiscriminate mass or group, treat as alike without regard for particulars
Consistent with our emphasis on the importance of being aware of the importance of how assessment literacy differs within different contexts and positions, we feel that it is also critical to improving assessment literacy to explicitly acknowledge the distinct skills sets that assessment literacy comprises.  There may be other classification schemes that work well (e.g., those that include communication skills), but for this framework, we propose the following three distinct sets of skills as fundamental components of assessment literacy.

Testing Literacy – Understand the fundamental principles of test design, development, and use.
Measurement Literacy – Understand the fundamental measurement principles, particularly those related to validity and the uncertainty of measurement.

Data Literacy – Possess the basic skills needed to organize and manipulate data so that it can be analyzed, interpreted, and used appropriately.

The term testing literacy refers to skills directly related to educational testing for the purpose of distinguishing those skills within the overall topic of assessment literacy.  The synonymous use of the terms test and assessment to describe an instrument or testing and assessment to describe a process in general practice can cause confusion when attempting to discuss these three components of assessment literacy.

Among the three categories, data literacy is probably the category least often associated with assessment literacy.  As educators’ use of both formal and informal assessment data becomes more commonplace, however, educators must be comfortable using available tools to perform routine tasks such as sorting and filtering data, generating desired reports, and combining data across a variety of sources.

There is obviously overlap across the three categories.  The categorization does not imply that the principles in each of the categories should be addressed separately or out of context.   However, educators’ understanding of assessment concepts and the development of instructional materials can be improved by a better understanding that assessment literacy is multi-dimensional, requiring a knowledge and skills from a variety of disciplines.  When developing instructional materials for assessment literacy, a first step should be to identify key particular knowledge and skills within each cell.

![Assessment Literacy Framework](https://charliedepascale.files.wordpress.com/2016/02/framework.jpg)

## Improving Assessment Literacy

The following sections address several aspects that all educators should know in order to be assessment literate and conversant with assessment topics.  In particular, educators need to understand the role of assessments and be knowledgeable consumers of assessment information, be constructors of high-quality assessments and scoring guides that reflect important thinking and problem solving skills, and be able to interpret and use the results of students’ performances.

### The Role of Assessments and Assessment Information

Teaching and assessing are two sides of the same student learning coin.  They cannot be fully separated, nor should they be.  In other words, assessment need not be viewed as an official event and separate from instruction.  In fact, good assessment tasks can be interchangeable with good instructional tasks (Shepard, p. 8).  In a review of studies related to assessment, Black and Willam (1998) found that “the research suggested that attention to the use of assessment to inform instruction, particularly at the classroom level, in many cases effectively doubled the speed of student learning.”  Since assessments are such powerful tools that when used effectively can give educators the vital information they need to make instructional decisions that are grounded in student learning. However, to take full advantage of the power of assessment, teachers must understand what information can be learned, not just from standardized state tests, but from the results of all assessments, whether gathered through a formative process, district- or state initiated interim assessments, summative assessments, informal or formal assessments.  Assessment literate educators can clearly distinguish the role of different types of assessments.  Summative, interim, and formative assessments are all critical components of an educator’s repertoire in order to determine the academic strengths and needs of students, as well as to determine instructional next steps.  

Consider the following definitions of the types of assessments:

Formative Assessment:  A process that teachers and students use to gather information during, as opposed to after, the learning process and to make instructional adjustments accordingly.

Interim Assessments:  Assessments administered during instruction that are designed to evaluate students’ knowledge and skills relative to a specific set of goals to inform decisions in the classroom and beyond.  Interim assessments have the distinct quality of being used to predict a student’s future performance on a specified summative assessment. {Note:  the definitions of interim assessments vary.}

Summative Assessments:  Formal assessments that are given at the end of a unit, term, course, or academic year.  
Since the same assessment can be used both formatively and summatively, it is important for educators to note that an assessment cannot be categorized as one or the other, but are rather categorized by determined by their purposes.  When assessments are used formatively (to inform instruction) teachers can make thoughtful decisions about what instructional strategy is most appropriate, such as asking questions, providing examples, modeling, scaffolding, as well as determining when to move on to the next concept.  In addition, formative assessments can help students to transfer their learning to new situations when asked about previously learned understandings in new ways, having students draw new connections based on their prior knowledge, and allowing students to monitor their own learning.  Assessments used for making instructional decisions must be frequent and ongoing, making assessment the central process in instruction.  On the other hand, summative assessments, including standardized accountability tests, are used as determinations of whether students are meeting the required standards.  Interim benchmark assessments provide evidence of student learning that is comparable across classrooms revealing whether students are mastering standards and the progress they are making over time.

As administrators, districts, and states identify required assessments, they should be clear about the intended purpose(s) of the assessment, as well what information they intend to gain from it.  Many district-driven assessments are used to gauge student progress and help teachers improve instruction. Districts and teachers should collaborate to determine which assessments are useful to their instructional practice when evaluating their portfolio of assessments and determining which tests to keep or eliminate so that there are not duplicative efforts.  These considerations are outlined in four key principles that educators should reflect on when developing or selecting assessments:

- The results of assessments should be beneficial for students (e.g., gain services for students with special needs, to inform instruction by building on what students already know, to improve programs, etc.).
- The content of the assessments should allow students to demonstrate progress toward important learning goals and be aligned with the subject matter.
- The selected assessments should fit the identified purpose.
- A variety of assessment types should be utilized to measure student achievement.


### High Quality Assessments and Scoring Guidelines

High-quality assessments generate rich data and can provide valuable information about student progress to teachers and parents, support accountability, promote high expectations, and encourage equity for all students.  Used appropriately, high-quality assessments can be a valuable tool for teachers to determine where students are struggling, for parents to understand their children’s progress and knowledge gaps, and for policymakers who need assurance that all students are receiving a high-quality education.  

In order for educators to select or create high quality assessments, they should understand the concepts of content-related validity evidence, fairness, reliability, and appropriate scoring, as specific assessment related issues.  An assessment which has content-validity should provide an accurate picture of what students know, understand, and are able to do.  This requires that it is aligned to grade appropriate content and the intended level of cognitive rigor.  A fair assessment ensures that students are measured only on the basis of knowledge and skills being measured and is free from bias that can distort results.  A reliable assessment includes enough items in which to gather sufficient evidence that provides a consistent picture of what students know, understand, and are able to do no matter who scores the assessment.  This of course means that the scoring guidelines or rubrics, which are used to evaluate the quality of students’ constructed responses or performances, are focused on two essential questions:  1) What do we want students to know and do? And 2) What would exemplary demonstration of this learning look like. Therefore, when selecting or creating a rubric, the evaluative criteria should be focused on the critical knowledge and skills that are being assessed, and the descriptors should be specific enough for educators to ascertain students’ strengths and areas of need in order to make instructional decisions.

When developing and/or selecting the assessments for classroom progress monitoring, it is important for educators to think about the following questions as they will help to ensure that high quality assessments are being used:
-	What are the most appropriate types of assessment items for measuring the learning (e.g., multiple choice, performance or essay tasks)?
-	Is the assessment aligned to the standards or learning outcomes?  Consider whether the assessment will actually measure what is intended to be measured.
-	Is the assessment appropriately rigorous and have an appropriate level of difficulty?
-	Are the directions and vocabulary clear, ensuring that they don’t detract from what students know and are able to demonstrate?
-	Do the rubrics have appropriate evaluative criteria and quality descriptors?


## Interpretation and Use of Student Assessment Data

The increased use of a variety of assessments, as well as more sophisticated technology, has made more data available in schools than ever before.  This access to current and varied student learning data has been described as “teaching with the lights on” because educators do not have to guess what students know or hope that their instruction is having the desired effect.  Data provide a way to confirm what students are learning and the extent to which they are making progress towards goals and targets.  Though assessments allow for the collection of this information, they are only as valuable as the quality of the information collected and the way that the information is used.  

Using data systematically, whether running records, observations, response logs, performance assessments, or quizzes, to ask questions and gain insight about student progress is a logical way to tailor instruction to meet the needs of all students.  Using the information that data provides allows educators to make decisions aimed at improving student achievement, such as:
-	prioritizing instructional time
-	targeting struggling or high-performing students to provide additional and individualized instruction
-	identifying individual students’ strengths and needs to provide appropriate interventions
-	gauging the instructional effectiveness of classroom lessons
-	refining instructional strategies
-	examining school-wide data to determine how to adapt curriculum

Data can include demographic and behavioral criteria that provide information about students’ background, attendance, social and behavioral issues, mobility, retention and dropout rates (Learning Points Associates, 2004), achievement data from interim benchmark assessments, curriculum-based assessments, including performance assessments, classroom activities, tests, quizzes, student report card grades, as well as state tests.  When data are used appropriately and in conjunction with each other, educators can create a rich picture of students’ school experiences and learning progress.

The best way to illustrate how the different structures and opportunities that are created in schools and can support the interpretation and application of data-informed decision throughout a school year is by examining a case studies at Barley Mill Middle School in which the staff used their assessment review as a means analyzing students’ strengths and needs in order to making instructional decisions.

## Sustaining and Evaluating Assessment Literacy

In the final section of this paper will we discuss recommendations for sustaining and evaluating assessment literacy initiatives.   We argue that literacy efforts are most likely to be sustained and successful when characterized by four factors:

1.	Relevant Focus: Literacy initiatives are thoughtfully designed to be meaningful and useful for a variety of practitioners
2.	Support Structures:  There are well defined mechanisms to provide the appropriate resources to support literacy initiatives   
3.	Widespread Investment and Agency:   There is buy-in from a variety of stakeholders at multiple levels, who are equipped and empowered to maintain and expand initiatives  
4.	Ongoing Monitoring and Review:  Any worthwhile initiative must be regularly monitored and evaluated to foster ongoing improvement

These factors are neither linear nor mutually exclusive.  A thoughtful strategy to sustain and support literacy involves continuing and often concurrent attention to each element.  Because these factors are closely related, we expect that efforts to support  any one element, will positively impact all others.    Conversely, inattention to any one factor may broadly threaten sustainability of the entire enterprise.
Relevant Focus

As discussed in previous sections, literacy is multifaceted.  Our framework emphasizes that different users need different information for different purposes.    If educators and leaders see literacy initiatives as disconnected from their work, it is unlikely to encourage the support necessary to sustain it.   Too often assessment literacy is regarded as a “psychometric exotica” that is unrelated to everyday decisions of educators (Popham, 2006).  To create sustainable and effective literacy programs, substantial up-front effort is essential to insure the focus is relevant.
We believe our framework can be useful to help determine and prioritize the literacy topics that are most critical.  This starts by soliciting feedback form a variety of stakeholders about the skills that are most essential to their success.  Rather than focusing exclusively on what stakeholders want to know, it is vital to solicit feedback about what they want to do.  That is, how do stakeholders want to use assessment information to support prioritized their goals. Potential questions to ask may include:

-	What questions do you want to answer with information about student achievement?
-	In what ways do you want to use data to inform decisions about individual students, practices, and/or policies?   
-	What information do you typically use to support these decisions? What are the strengths and limitations of your current practices?
-	What outcomes are most important to promote?

We propose a focus on purposes and uses to hone-in on the information and practices that are likely to be most relevant to various users.  Consider, for example, that the assessment literacy needs for elementary teacher working to understand and promote progress in English proficiency for young language learners, is likely very different from the literacy needs of a high-school math department head seeking to evaluate the effectiveness of her school’s college-preparatory curriculum.   To the extent the literacy initiative is tailored to meet the needs of a broad range of practitioners, it is much more likely to be supported and sustained.
This focus on relevance can be supported by ‘traditional’ data collection approaches such as surveys or focus-groups.   To be most effective, however, these mechanisms should elicit responses about desired purposes and uses and facilitate association of responses to the respondent’s roles and responsibilities.  In this manner, implementation of sustainable literacy initiatives will be supported by a better understanding of topics that are likely to be relevant and useful.
Support Structures

Support structures refer to the range of requirements necessary to operationalize and sustain a high-quality literacy initiative.   We associate requirements with one of two categories for the sake of presentation: organizational support and implementation resources.  
Organizational support primarily refers to the personnel and policy requirements that accompany a thoughtful plan to implement and sustain the assessment literacy initiative.  It starts with having personnel who are explicitly charged with design and oversight.  These personnel should have sufficient authority to carry-out the plan and secure the resources necessary to implement it.  Ideally, at each important level in an organizational structure (e.g. district, complex, school), there will be one or more persons with the authority and responsibility for implementation.  As we will discuss later in this section, broad-based 'bottom-up' empowerment is critical as well, but it will likely be most effective if the assessment literacy initiative is clearly supported from the 'top-down'.

Organizational leadership should also provide the policies and clarity of expectations necessary to support the success of assessment literacy.  Siggins (1995) asserts, “the key planning question is: What policies will need to be changed or added to our district’s policy manual to support a positive assessment and learning environment.”    Such policies may involve requiring pre-service or in-service training, encumbering time and resources for planning and development, or adding specific demonstrations to educator performance criteria.

Additionally, there are a range of resources that are critical to implementing and sustaining an assessment literacy program.   Expertise is by far the most valuable commodity and resources should be provided to better develop and share ideas.   These may include: access to relevant literature, participation in conferences, support from experts within and outside the organization, and, of course, time to study, plan, and collaborate with colleagues.

Although face-to-face collaboration is always valuable, leveraging technology has tremendous potential to help develop, spread, and sustain assessment literacy expertise.  Naturally communication technology provides numerous avenues for sharing ideas via collaboration software and enterprise social media, for example.  Moreover, technology can be leveraged to share ideas in manner that best ‘fits’ consumers such as by narrated slides, videos, or interactive graphs to name only a few of the countless alternatives.  We believe that nominal investments in building capacity to use these tools will yield great dividends in the long run, as stakeholders leverage the power of these resources to develop and share ideas efficiently.        
Widespread Investment and Agency

Perhaps the most important element for developing and sustaining an assessment literacy initiative is to create buy-in from a broad group of stakeholders and empower them to maintain and expand the work.  Stated simply, an approach based on mutually exclusive categories of ‘producers’ or ‘consumers’ of knowledge is short-sighted and unsustainable.    Far more effective is a model where all participants are both eager learners and able teachers.

This starts by working to create buy-in from everyone in the community. As indicated previously, we believe this arises naturally when the focus of assessment literacy is demonstrably relevant for diverse groups of leaders, educators, and other stakeholders serving many important roles with different interests.  It is further augmented when participants have a voice in determining the focus and manner of their learning.  While some people may respond well to more traditional professional development sessions, others may wish to pursue continuing education through external providers or an online course, and still others may learn best by collaborating on a project.  As long as the objectives and the outcomes of learning are clear and mutually agreeable, we see no reason to limit the variety of approaches that one pursues.  In fact, by expanding the very definition of ‘professional development’ to include experiences that are likely to be more engaging, the potential to motivate learners and create buy-in is augmented.

As noted, an important objective of learning experience should be to share that expertise with others.  As anyone who has ever taught a course, written a paper, or otherwise worked to share knowledge with others can attest, it is a valuable learning experience for the producer and the consumer.  By so doing, not only can organizations produce ever-growing banks of work products (e.g. courses, papers, videos, etc.), creating more capacity to share expertise can  exponentially increase the reach.  Popham (2006) imagines, “As assessment literacy promulgators, measurement-knowledgeable educators could spread the word.  The truth is, we need piles of such promulgators.”  


### Ongoing Monitoring and Review

Finally, one of the most important considerations of a comprehensive assessment literacy plan is a well-structured plan to monitor and review the program and make improvements as appropriate.    By so doing, organizations can maximize the likelihood that their objectives are being met and minimize unintended negative consequences.

An important starting point for an evaluation plan is a well-articulated set of goals for the program and a theory of action that explicates how those goals are thought to be supported.  A comprehensive theory of action should also indicate the conditions or requirements necessary to produce the intended outcomes.  While the goals and associated theory of action likely differ for various organizations and with distinct needs, we present some overarching evaluation questions we believe should be addressed in most any evaluation process and examples of evidence that may be collected to inform the question.  

### Evaluation Question	Examples of Evidence

To what extent are the objectives of the initiative clearly defined and well-known?	-	Documentation of program objectives
- Record of communication  strategies to disseminate information
- Results from surveys and/or focus groups

Are the pre-requisite conditions (i.e. inputs) and resources appropriate to support the intended outcomes?

- If pre-requisite knowledge or skills are required, records of training and/or results from assessments to certify successful completion
- If specific resources (e.g. literature, technology) are required, audits to verify accessibility and surveys or observations to gauge the extent to which key personnel have access to the resources, are using them as intended, and report it is effective


To what extent do specific assessment literacy programs or resources serve their intended purposes?

- Data on frequency of use
- Feedback that describes manner of use that is consistent with intended use
- Qualitative and quantitative feedback from consumers regarding perceived satisfaction and benefit


Is the assessment literacy program building capacity as intended?
- Self-report feedback from participants regarding use and benefit
- Increase in quality and quantity of desired behaviors (e.g. improved educator evaluation scores in specific areas, reports from parent, staff, or student surveys)
- Increase in quality and quantity of specific work products (e.g. quality of assessments produced, number and quality of planning sessions with colleagues using assessment data)
- Experimental or quasi-experimental design that compares demonstration of target competencies for participants versus non-participants factoring in degree of participation

Does the assessment literacy initiative appear to have a positive impact on student achievement?
- Evidence of improved student outcomes (e.g. growth data)
- Experimental or quasi-experimental design comparing target student outcomes for organizations/ environments characterized by high degree of desired assessment literacy practices with a suitable control condition

To what extent are unintended negative outcomes evident?

- Feedback from surveys, focus groups, or other mechanisms identifying the presence and degree of negative consequences
- Data suggesting increase frequency of negative outcomes (e.g. higher attrition) in high implementation areas

The nature of a comprehensive assessment literacy plan described in this document requires evidence from multiple sources to more fully gauge impact.  For example, the program may be more effective for participants in one role compared to another or more effective to support a certain use compared to another.  For this reason, the source and nature of the data collected should be frequent and varied.  The best case for drawing conclusions occurs when 1) multiple and distinct sources of evidence (e.g. focus group, survey, and data analyses) support the same finding 2) degree or magnitude of that finding is practically meaningful and 3) it occurs repeatedly (e.g. over multiple years for multiple users or units).  Of course, assembling and evaluating this type of evidence takes patience, discipline, and a commitment to short and long-term data collection and evaluation sustained by systemic support.  

## References

American Association of School Administrators, Using Data to Improve Schools:  What’s Working, Retrieved from http://aasa.org/uploadedFiles/Policy_and_Advocacy/files/UsingDataToImproveSchools.pdf. 

DeWitt, P. (2011).  Why We  Need to Differentiate Between Assessment and Testing.  Education Week, http://blogs.edweek.org/edweek/finding_common_ground/2011/12/why_we_need_to_differentiate_between_assessment_testing.html?preview=1&r=1861573305. 

Greenberg, J., Walsh, K., & McKee, A. (2013).  Teacher Prep Review:  A review of the nation’s teacher preparation programs. National Council on Teacher Quality, http://www.nctq.org/dmsView/Teacher_Prep_Review_2014_Report . 

Hamilton, L, Haverson, R., Jackson, S., Mandinach, E., Supovitz, J., Wayman, J. (2009) Using student achievement data to support instructional decision making (NCEE 2009-4067).  Washington, D.C.: National Center for Education Evaluation and Regional Assistance, Institute of Education Sciences, U.S., Department of Education.  http://ies.ed.gov/ncee/wwc/publications/practiceguides/

Herman, J. L., Osmundson, E., Ayala, C., Schneider, S., & Timms, M. (2006). The nature and impact of teachers' formative assessment practices. CSE Technical Report #703. National Center for Research on Evaluation, Standards, and Student Testing (CRESST).

Lazarin, M. (2014).  Testing Overload in America’s Schools.  Center for American Progress, https://www.americanprogress.org/issues/education/report/2014/10/16/99073/testing-overload-in-americas-schools/. 

Learning Points Associates (2004). Guide to using data in school improvement efforts: A compilation of knowledge from data retreats and data use at Learning Points Associates. Naperville, Ill.: Author. Retrieved from http://www.ncrel.org/datause/howto/guidebook.pdf.

Marsh, C. J. (2007). A critical analysis of the use of formative assessment in schools. Educational Research and Policy Practice, 6, 25–29.

McMillan, J. H. (2002).  What Teachers Need to Know about Assessment.  National Education Association, http://echo.edres.org:8080/nea/teachers.pdf. 

Michigan Assessment Consortium (2014).  Assessment Literacy Standards: An Imperative for Michigan.  Retrieved from http://michiganassessmentconsortium.org/sites/default/files/mac_AssessLitStds_mobile.pdf.

Perie, M., Marion, S., & Gong, B. (2009), Moving Toward a Comprehensive Assessment System:  A Framework for Considering Interim Assessments, National Council on Measurement in Education.

Popham, W. J. (2011). Assessment Literacy Overlooked: A Teacher Educator's Confession.  Teacher Educator, v46 n4 p265-273.

Popham, W. J. (2009).  Unlearned Lessons: Six Stumbling Blocks to our Schools' Success.  Cambridge, Massachusetts: Harvard University Press.

Popham, W. J. (2006).  All About Accountability/Needed:  A Dose of Assessment Literacy.  Educational Leadership, vol. 63, no. 6, pp. 84-85.

Stiggins, R. J. (1995). Assessment Literacy for the 21st Century. The Phi Delta Kappan, (3). 238.



 
## Case Study 1

Barley Mill Middle School is located in a rural area with slightly more than 100 students and a total of 10 teachers representing all content areas.  There is one other middle school in the district which has slightly fewer than 100 students.  Approximately half of the eighth grade students at Barley Middle School demonstrate proficiency on the 8th grade state test in both math and ELA with the other half demonstrating nearly proficient scores.  Test scores, however, have remained relatively stagnant for over five years.  The staff, as a whole, has not focused on the use of data for instructional decision-making as they felt that they were doing a "good job".

The recently hired superintendent of the district has embraced the concept of being a data-informed district.  After the initial grumbling subsided, the principal, Howard Franklin, and his staff decided that perhaps that it was time to make a change in the way they do business.  Mr. Franklin and the teachers embarked on reading about data use and decision-making and began to view this as an opportunity to work together as a staff toward improving student achievement.

During a faculty meeting early in October, Mr. Franklin and the teachers, led by a data coach from the district office, began pouring over the state assessment data from the previous three years to identify specific areas of strength and areas of need.  They quickly came to the realization that students conceptually understood and could demonstrate content knowledge that required low levels of cognitive rigor.  Specifically, students were able to do relatively well on multiple choice and short answer responses.  However, students struggled on constructed response items, for which they needed to analyze information and provide support and evidence, draw conclusions based on evidence provided, or solve multi-step problems.  This realization led the teachers to begin to seriously examine the assessments they were developing and using in their own classrooms, as well as the results they were yielding.

At a subsequent faculty meeting, teachers came with their “typical” assessments and samples of student work and began to compare what was being used and how students were performing.  Ms. Wilkes and Mr. Johnson, both English teachers of each of the middle school grades, shared that they use a combination of multiple choice questions and a literary analysis prompt for assessing student understanding of the novels read.  When examining the student work, Mr. Johnson was the first to vocalize that students do quite well on the multiple choice part of the assessments, which requires recollection of discrete facts about the literary elements in the text, but do mediocre to poor on the literary analysis.  When these teachers were probed further on the weighting of the literary analysis there was a realization that it was only worth ten percent of the entire assessment grade.  Consequently, students were receiving high scores on the work they were producing in their classes.  As the teachers looked across content areas, the same information continued to be apparent.  In fact, they soon realized that there were some courses in which students were not expected to construct responses to questions or the quality of the communication was not scored.   By the end of this meeting, the teachers had identified informational writing as their shared area of focus.

Since Barley Mill Middle School is a small school with one or two teachers per content area, they had been unable to implement a common planning time for teachers.  Through a brainstorming session, the staff agreed to meet as a humanities department (English and social studies), a math/science department, and specialists department (physical education and fine arts) one hour a week in the morning prior to students arriving.  Initially, they planned to use these meetings to develop a common informational writing rubric and to review each other’s constructed response assignments to ensure full alignment to the curriculum in both content and cognitive rigor.  Within a month, the focus changed to scoring student work in teams, analyzing student results, annotating student work, and discussing instructional decisions.  With the change of focus, the teachers researched a variety of protocols and practiced using them to in order to help them gather the information that would be most beneficial for improving student achievement.  The protocol they agreed upon using included the criteria of ensuring consistency in scoring and reaching consensus about what constitutes a proficient piece of writing for the assignment, identifying the strengths and needs of the high, average, and low student groups, as well as identifying instructional needs for the whole class and for each group of students.

In the midst of this process, Ms. Fowler, the visual arts teacher, suggested that the different departments begin to understand what informational writing “looked” like in each class.  The teachers began to reorganize their morning meetings in a variety of different ways, including different content areas and teachers with the same students meeting together.  In addition, several teachers agreed to observe the teaching of informational writing in different classes.

By the end of March, just prior to the state test, teachers met at a faculty meeting to discuss how their instruction and assessments had changed, and how students were performing on constructed responses.  They were pleased with the overall improvement in student writing that had occurred during the school year.  They felt confident that more students would demonstrate greater proficiency on the state test; but more importantly, as Mr. Sampson shared, “The way I approach teaching and assessing students has changed through this process of analyzing student work and data.  I don’t know how I didn’t consider the results of student work to make instructional decisions all these years!”

## Case Study 2


Abington Elementary School is a Title I school located in an urban location.  Abington has approximately 600 students attending full-day kindergarten through fifth grade.  There are approximately six teachers at the primary level (K-2) and five teachers for third through fifth grades.  Abington Elementary School has been making steady progress in reading over the past three years under the leadership of Suzanne Martin.  Her motto has been “what’s best for students is best for us.”  Ms. Martin and the teachers have made informed decisions about teaching reading which includes using a 4-block model for English Language Arts (ELA) instruction with a strong focus on a guided reading approach.  Within this approach, each teacher has approximately 120 minutes of ELA instruction every day with four flexible groups of students for direct reading instruction.  Within these groups, students demonstrate similar reading behaviors and can read similar levels of texts.  The teachers use a variety of assessment data, from informal assessments (checklists of reading behaviors and running records) to interim assessments (DIBELS and DRA2), in order to make decisions about the composition of these groups of students, so consequently the guided reading groups can change on an as needed basis based on the strengths and needs of each student.  In addition, the district has developed a series of local reading and writing assessments that are aligned to the curriculum at each grade level.  Reading assessments include such skills as reading high-frequency words, breadth of vocabulary (e.g., identifying homophones, multiple meaning words, etc.), and language structures.  Text-dependent analysis questions are administered on a quarterly basis as are narrative, informational, and opinion writing prompts.  Each assessment is accompanied by a rubric and because the culture of the school has been focused on making data-informed decisions, faculty meetings and professional development sessions are utilized to calibrate the scoring of student work and to discuss instructional decisions; yet to understand how the classes are formed and how the groups are structured, the planning actually begins before the start of a new school year.

At the end of each school year, Ms. Martin distributes a color-coded (by grade) half sheet of card stock paper that includes general information about each student and a table for recording academic information.  Each teacher reports identifying and behavioral information, guided reading level based on Fountas & Pinnell Leveled Texts (Heinemann) and whether the student demonstrates that he/she is at the high (H), average (A), or low (L) end of proficiency within the level, based on the student’s reading fluency and comprehension ability.  The teacher also records her/his students’ scores on the text-dependent analysis questions and writing prompts.  Although the focus has been on ELA, the faculty is also carefully monitoring math achievement, as well as students’ ability to demonstrate science and social studies content knowledge.  Student scores for district-developed performance assessments aligned to these content areas are also recorded in the table.

###### Student Name:
###### Gender:
###### Special Needs:
###### Record the student’s academic information below:
###### Reading Level and (H, A, L)	TDA	Writing	Math Science Social Studies
###### Q1						
###### Q2						
###### Q3						
###### Q4						
###### Behavioral Concerns (specific incidents, specific students to avoid placing in the same class, etc.):
###### Parental Concerns:
###### Other information:

The final faculty meeting of the school year is used for grade level teachers to meet together and group students based on the information recorded on the cards.  Careful consideration is given so that students are provided with opportunities to successfully engage with a variety of their peers throughout their elementary school experience.  Finally, Ms. Martin, the assistant principal, and the reading specialist review the groupings and make the final decisions for placing the students with individual teachers.

Prior to the beginning of the upcoming school year, the classroom teacher is able to access his/her student information, including viewing the actual assessments which are collected in a student portfolio.  However, the first back-to-school day is designated for reviewing this information and having vertical team discussions about students and instructional decisions.  The goal for the staff at Abington Elementary School is to begin the year knowledgeable about each student’s ability, strengths, and needs based on the assessment data so that there is no time wasted in beginning instruction.

## Conclusion

Assessment entails a collection of procedures that inform the learning process. Formative, interim, and summative assessments each have a place in the larger system of assessment, instruction, and curriculum. When formative assessments are used in conjunction with interim and summative assessment, the potential exists to improve outcomes for all students. Assessments can only serve this purpose, however, when teachers are supported to make appropriate adjustments in their instruction (Herman et al., 2006; Marsh, 2007).

Formative assessment can be most directly used at the individual student level because it measures how a particular student is progressing in the instructional program and identifies where support or enrichment may be needed. Focusing on the individual provides immediate feedback to the student and teacher on the student’s progress within the curriculum. Formative assessment may also be evaluated at the classroom level to inform teaching practices because it reveals how many students may be experiencing difficulty. If many students are having difficulty, then perhaps a more general change in instruction is needed.

Interim assessment data can provide teachers with information about what concepts students have learned and the potential to provide follow-up for struggling students.   Interim assessments can be analyzed and used to provide feedback to students, to allow for the re-teaching of necessary foundational skills or concepts, differentiating instruction, and rethinking the way in which a concept was taught.  They can provide a structured and systematic strategy for examining overall achievement and to identify areas of need that may be overlooked in everyday classroom interactions.

Summative assessment informs instructional practices in a different yet equally important way. Critics of large-scale assessments argue that they are disconnected from instruction and are not useful in the instructional process (Shepard, 2001). However, summative assessment can serve both as a guide to teaching methods and to improving curriculum to better match the needs of the students. A primary use of assessment data is in planning curricula. For example, if a school's performance on a state assessment indicates high percentages of students who do not meet standards in writing, then the school could collect more information on its writing curricula, student writing performance (through portfolios or other classroom work), and professional development needs for its teachers. After collecting such information, the school may then review and adopt new writing curricula as well as provide professional development to its teachers in order to support stronger student achievement in writing. Ongoing evaluation of the writing program would be conducted through the use of formative, interim, and summative assessment. In this manner, when summative and formative assessments are aligned, they can inform the instructional process and support both the daily instructional practices of teachers as well as the longer-term planning of curricula and instruction.
